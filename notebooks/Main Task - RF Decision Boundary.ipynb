{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../src'))\n",
    "import document_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_PATH = '../classifiers/RF_by-article_stats-and-counts.pickle'\n",
    "DOC_ENCODER     = '../generated_datasets/DocEncoder_by-article_stats-and-counts.pickle'\n",
    "DATASET_PATH    = '../generated_datasets/by-article_stats-and-counts.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = pickle.load(open(CLASSIFIER_PATH, 'rb'))  ## Random Forest Classifier\n",
    "encoder = pickle.load(open(DOC_ENCODER, 'rb'))\n",
    "counter = encoder.counter  ## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Invert word->idx mapping to get features' names\n",
    "idx_to_word = {idx: word for word, idx in counter.vocabulary_.items()}\n",
    "word_feature_names = [idx_to_word[i] for i in sorted(idx_to_word.keys())]\n",
    "\n",
    "statistical_feature_names = [\n",
    "    'num_sentences',       ## Number of sentences\n",
    "    'avg_sent_word_len',   ## Average sentence length in words\n",
    "    'avg_sent_char_len',   ## Average sentence length in chars\n",
    "    'var_sent_char_len',   ## Variance of sentence length in chars\n",
    "    'avg_word_len',        ## Average word length\n",
    "    'var_word_len',        ## Variance of word length\n",
    "    'punct_freq',          ## Frequency of punctuation\n",
    "    'capital_freq',        ## Frequency of capital letters\n",
    "    'ratio_atoms_to_types' ## Types to atoms ratio\n",
    "]\n",
    "\n",
    "feature_names = statistical_feature_names + word_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(DATASET_PATH)\n",
    "X, y = data['X'], data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07098779 0.03089282 0.03175361 0.03549271 0.03012492 0.03813169\n",
      " 0.03928453 0.0453966  0.04400098 0.02051239 0.03893597 0.00742571\n",
      " 0.01225441 0.00461132 0.01265352 0.00978777 0.00850962 0.00760899\n",
      " 0.00943164 0.00852886 0.00765101 0.00630694 0.00381572 0.00464075\n",
      " 0.01347229 0.00917908 0.00669274 0.00522424 0.01901321 0.0104065\n",
      " 0.01493992 0.02267638 0.00512259 0.00948857 0.00877956 0.01144004\n",
      " 0.01083058 0.0115706  0.00659671 0.01233621 0.00641018 0.03302811\n",
      " 0.03064863 0.01109274 0.00887792 0.01149064 0.01460757 0.01150031\n",
      " 0.0067093  0.01234446 0.01576297 0.00787356 0.00808254 0.0355903\n",
      " 0.03026035 0.01584296 0.00913141 0.01011466 0.01412192]\n",
      "['num_sentences', 'capital_freq', 'ratio_atoms_to_types', 'punct_freq', 'american', 'var_word_len', 'trump', 'var_sent_char_len', 'polit', 'avg_sent_char_len', 'avg_sent_word_len', 'presid', 'use', 'avg_word_len', 'make', 'america', 'just', 'want', 'thing', 'like', 'said', 'year', 'hillari', 'countri', 'support', 'peopl', 'clinton', 'obama', 'say', 'right', 'new', 'report', 'news', 'know', 'work', 'day', 'media', 'donald', 'hillari clinton', 'white', 'republican', 'nation', 'donald trump', 'democrat', 'time', 'think', 'dont', 'did', 'attack', 'state', 'hous', 'offic', 'polic', 'elect', 'investig', 'man', 'fbi', 'come', 'email']\n"
     ]
    }
   ],
   "source": [
    "features_by_importance = list(reversed(np.argsort(rf_clf.feature_importances_)))\n",
    "print(rf_clf.feature_importances_)\n",
    "print([feature_names[i] for i in features_by_importance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureA = features_by_importance[2]\n",
    "featureA_name = feature_names[featureA]\n",
    "featureB = features_by_importance[4]\n",
    "featureB_name = feature_names[featureB]\n",
    "\n",
    "a_min, a_max = X[:, featureA].min(), X[:, featureA].max()\n",
    "b_min, b_max = X[:, featureB].min(), X[:, featureB].max()\n",
    "\n",
    "a_diff, b_diff = a_max - a_min, b_max - b_min\n",
    "aa, bb = np.meshgrid(\n",
    "    np.arange(a_min - a_diff / 5, a_max + a_diff / 5, a_diff / 20),\n",
    "    np.arange(b_min - b_diff / 5, b_max + b_diff / 5, (b_max - b_min) / 20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_avg = np.median(X, axis=0)\n",
    "\n",
    "X_synthetic = np.array([\n",
    "    [a if i == featureA else b if i == featureB else X_avg[i] for i in range(len(X_avg))] \\\n",
    "    for a, b in np.c_[aa.ravel(), bb.ravel()]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_clf.predict(X_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
