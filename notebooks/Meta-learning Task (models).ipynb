{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook for the Meta-Learning sub-task of the SemEval2019 Task on \"Hyperpartisan News Detection\".\n",
    "\"\"\"\n",
    "TRAIN_DATASET_PATH = '../meta-learning-task/pan19-hyperpartisan-news-detection-by-article-meta-training-dataset-2019-02-04/'\n",
    "GROUND_TRUTH_PATH  = '../meta-learning-task/pan19-hyperpartisan-news-detection-by-article-meta-training-dataset-2019-02-04/ground-truth/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "data = dict()\n",
    "\n",
    "for file in os.listdir(TRAIN_DATASET_PATH):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(TRAIN_DATASET_PATH + file) as in_file:\n",
    "            reader = csv.reader(in_file, delimiter=' ') ## csv with space delimiters\n",
    "            data = {row[0]: [(w == 'true') for w in row[1:]] for row in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = dict()\n",
    "\n",
    "for file in os.listdir(GROUND_TRUTH_PATH):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(GROUND_TRUTH_PATH + file) as in_file:\n",
    "            reader = csv.reader(in_file, delimiter=' ') ## csv with space delimiters\n",
    "            truth = {row[0]: (row[1] == 'true') for row in reader}\n",
    "            \n",
    "assert len(data) == len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (420, 42) ; X_vote.shape: (420, 43)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([data[key] for key in sorted(data.keys())], dtype=np.bool)\n",
    "y = np.array([truth[key] for key in sorted(data.keys())], dtype=np.bool)\n",
    "\n",
    "## Add column with majority vote\n",
    "X_vote = np.average(X, axis=1)\n",
    "X_vote = np.reshape(X_vote, X_vote.shape + (1,))\n",
    "X_vote = np.concatenate((X, X_vote), axis=1)\n",
    "\n",
    "print('X.shape: {} ; X_vote.shape: {}'.format(X.shape, X_vote.shape))\n",
    "\n",
    "## Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vote, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Majority Vote Performance **\n",
      "\n",
      "Accuracy on whole dataset:\t 0.8214285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.81      0.82       210\n",
      "        True       0.82      0.83      0.82       210\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       420\n",
      "   macro avg       0.82      0.82      0.82       420\n",
      "weighted avg       0.82      0.82      0.82       420\n",
      "\n",
      "Accuracy on test dataset:\t 0.8571428571428571\n",
      "Accuracy on train dataset:\t 0.8125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "## Same function as baseline provided by SemEval\n",
    "majority_vote = lambda x: sum(x) >= ((len(x) - 1) / 2)\n",
    "\n",
    "y_pred = [majority_vote(x) for x in X]\n",
    "y_test_pred = [majority_vote(x) for x in X_test]\n",
    "y_train_pred = [majority_vote(x) for x in X_train]\n",
    "\n",
    "print('\\n** Majority Vote Performance **\\n')\n",
    "print('Accuracy on whole dataset:\\t', accuracy_score(y, y_pred))\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "print('Accuracy on test dataset:\\t', accuracy_score(y_test, y_test_pred))\n",
    "#print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print('Accuracy on train dataset:\\t', accuracy_score(y_train, y_train_pred))\n",
    "#print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Test score: 0.8547619047619047\n",
      "CV Test scores: [0.92857143 0.73809524 0.9047619  0.83333333 0.80952381 0.9047619\n",
      " 0.88095238 0.95238095 0.80952381 0.78571429]\n",
      "CV Train score: 0.8973544973544975\n",
      "CV Train scores: [0.88888889 0.9021164  0.8968254  0.8994709  0.9047619  0.89153439\n",
      " 0.8968254  0.8968254  0.8994709  0.8968254 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100, min_samples_leaf=3, random_state=42 ## min_samples_leaf=3 ?\n",
    ")\n",
    "\n",
    "## Train and cross-validate with 5 folds\n",
    "cv = cross_validate(rf_clf, X_vote, y, cv=10, return_train_score=True, scoring='accuracy')\n",
    "rf_cv_acc = sum(cv['test_score']) / len(cv['test_score'])\n",
    "\n",
    "print('CV Test score:', rf_cv_acc)\n",
    "print('CV Test scores:', cv['test_score'])\n",
    "\n",
    "print('CV Train score:', sum(cv['train_score']) / len(cv['train_score']))\n",
    "print('CV Train scores:', cv['train_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8690476190476191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00828001, 0.00925355, 0.00232323, 0.01154393, 0.03597662,\n",
       "       0.03466775, 0.01042077, 0.00412563, 0.02134086, 0.01786668,\n",
       "       0.15511596, 0.00945162, 0.00716676, 0.04166345, 0.0535333 ,\n",
       "       0.01684613, 0.0150044 , 0.03384055, 0.00474503, 0.00779191,\n",
       "       0.03363042, 0.05459089, 0.00638681, 0.01283064, 0.00359524,\n",
       "       0.00308327, 0.04835916, 0.00888776, 0.038552  , 0.00544551,\n",
       "       0.0145684 , 0.00599618, 0.00632205, 0.01504197, 0.01507528,\n",
       "       0.00661625, 0.00734167, 0.00376311, 0.00394275, 0.00463466,\n",
       "       0.00876711, 0.0332938 , 0.15831694])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)\n",
    "print('Accuracy score:', rf_clf.score(X_test, y_test))\n",
    "\n",
    "rf_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM\n",
      "TRAIN score: 0.8720238095238096\n",
      "TEST score: 0.8142857142857144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(max_iter=2000)\n",
    "cv = cross_validate(svm, X_vote, y, cv=5, return_train_score=True)\n",
    "\n",
    "print('Linear SVM')\n",
    "print('TRAIN score:', sum(cv['train_score']) / len(cv['train_score']))\n",
    "print('TEST score:', sum(cv['test_score']) / len(cv['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "TRAIN score: 0.8375\n",
      "TEST score: 0.8380952380952381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "cv = cross_validate(knn, X_vote, y, cv=5, return_train_score=True)\n",
    "\n",
    "print('KNN')\n",
    "print('TRAIN score:', sum(cv['train_score']) / len(cv['train_score']))\n",
    "print('TEST score:', sum(cv['test_score']) / len(cv['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andre/anaconda3/envs/python3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/andre/anaconda3/envs/python3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/andre/anaconda3/envs/python3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/andre/anaconda3/envs/python3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier\n",
      "TRAIN score: 0.8577380952380953\n",
      "TEST score: 0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andre/anaconda3/envs/python3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logist = LogisticRegressionCV(Cs=10)\n",
    "cv = cross_validate(logist, X_vote, y, cv=5, return_train_score=True)\n",
    "\n",
    "print('Logistic Regression Classifier')\n",
    "print('TRAIN score:', sum(cv['train_score']) / len(cv['train_score']))\n",
    "print('TEST score:', sum(cv['test_score']) / len(cv['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "TRAIN score: 0.9827380952380953\n",
      "TEST score: 0.8190476190476191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gboost = GradientBoostingClassifier(n_estimators=50, min_samples_leaf=5, min_samples_split=5, max_depth=5)\n",
    "cv = cross_validate(gboost, X_vote, y, cv=5, return_train_score=True)\n",
    "\n",
    "print('Gradient Boosting')\n",
    "print('TRAIN score:', sum(cv['train_score']) / len(cv['train_score']))\n",
    "print('TEST score:', sum(cv['test_score']) / len(cv['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "TRAIN score: 0.8738095238095237\n",
      "TEST score: 0.8047619047619048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, )\n",
    "cv = cross_validate(adaboost, X_vote, y, cv=5, return_train_score=True)\n",
    "\n",
    "print('Gradient Boosting')\n",
    "print('TRAIN score:', sum(cv['train_score']) / len(cv['train_score']))\n",
    "print('TEST score:', sum(cv['test_score']) / len(cv['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "### Random Forest classifier performs better than an MLP or LinearSVM, we'll continue with this classifier\n",
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
